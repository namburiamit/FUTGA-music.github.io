<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FUTGA:  Music Captioning Model</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <div class="container">
        <div class="header">
            <h1>FUTGA:</h1>
            <h3> Towards Fine-Grained Music Understanding through Temporally-Enhanced Generative Augmentation</h3>
            <div style="text-align: center;">
                <i>Junda Wu, Zachary Novack, Amit Namburi, Jiaheng Dai, Hao-Wen Dong, Zhouhang Xie, Carol Chen, Julian McAuley</i>
            </div>
        <div class="buttons">
            <a href="https://arxiv.org/pdf/2407.20445" class="button" target = "_blank">
                <span class="icon">ðŸ“°</span> Paper
            </a>
            <a href="video-link" class="button" target = "_blank">
                <span class="icon">ðŸŽ¥</span> Video
            </a>
            <a href="https://huggingface.co/JoshuaW1997/FUTGA" class="button" target = "_blank">
                <span class="icon">ðŸ¤—</span> HF Codebase
            </a>
        </div>

        </div>
        <div class="content">
            <h2>FUTGA: Future-Oriented Generative Algorithm for Music</h2>
            <!-- <div class="video-wrapper">
                <iframe src="link-tba" frameborder="0" allowfullscreen></iframe>
            </div> -->
            <h3>Abstract</h3>
            <p class ="content">Existing music captioning methods are limited to
            generating concise global descriptions of short music clips, which
            fail to capture fine-grained musical characteristics and
            time-aware musical changes. To address these limitations,
            we propose FUTGA, a model equipped with fine-grained
            music understanding capabilities through learning from
            generative augmentation with temporal compositions. We
            leverage existing music caption datasets and large language
            models (LLMs) to synthesize fine-grained music
            captions with structural descriptions and time boundaries
            for full-length songs. Augmented by the proposed synthetic
            dataset, FUTGA is enabled to identify the music's
            temporal changes at key transition points and their musical
            functions, as well as generate detailed descriptions for each
            music segment. We further introduce a full-length music
            caption dataset generated by FUTGA, as the augmentation of the MusicCaps and the Song Describer datasets.
            We evaluate the automatically generated captions on several
            downstream tasks, including music generation and re-
            trieval. The experiments demonstrate the quality of the
            generated captions and the better performance in various
            downstream tasks achieved by the proposed music captioning
            approach</p>
        </div>
    </div>
</body>

</html>